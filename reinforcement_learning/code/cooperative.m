% cooperative.m
% Jodie Simkoff

% This function takes in Q matrices and a sidewalk generated by
% QLearning_sidewalk_modules.m and integrates the modules so that they
% cooperate according to a best-average Q-value action policy.

function [ coop_paths, coop_scores] = cooperative(Q, sidewalk)

NUM_ITER = 10;
L = size(sidewalk,2);
W = size(sidewalk,1);

Qreward = Q{1}; 
Qobs = Q{2};
Qend = Q{3};

sidewalk_new = sidewalk;
startX = 1; startY = 4;
alpha = 0.9; gamma = 0.5;

    for iter =1:NUM_ITER
        if iter == 1
            figure();
        end
        sidewalk = sidewalk_new;
        score = 0;
        
        fig = imagesc(sidewalk); hold on;
        set(gcf, 'Position', [300, 300, 900, 400])
        set(gca,'xtick',[],'ytick',[]);
        title(iter)
        
        currX = startX ;
        currY = startY;
        pause(1)
        pathk = [startX startY];
        
        while currX < L
            
            pause(0.2);
            sidewalk(currY,currX) = 0;
            [m,action] = max(mean([Qreward(currY,currX,:), Qobs(currY,currX,:), Qend(currY,currX,:)]));
           
            prvX = currX; prvY = currY;
            
            [newX,newY] = move(currX,currY,action);
            
            currX = newX;
            if newY <= W && newY >= 1
                currY = newY;
            end
            
            pathk = [pathk; currX currY];
            
            if sidewalk(currY,currX) == 160
                reward = 3;
                obs = 0;
                isend = currX - prvX;
                score = score+3;
            elseif sidewalk(currY,currX) == 20
                reward = 0;
                obs = -1;
                isend = currX - prvX;
                score = score - 1;
            elseif sidewalk(currY,currX) == 200
                reward = 0;
                obs = 0;
                isend = 1000;
                
            else
                reward = 0;
                obs = 0;
                isend = currX - prvX;
            end
            
            
            Qreward(prvY,prvX,action) = Qreward(prvY,prvX,action) + ...
                alpha*(reward+gamma*max(Qreward(currY,currX,:)) - Qreward(prvY,prvX,action));
            Qobs(prvY,prvX,action) = Qobs(prvY,prvX,action) + ...
                alpha*(obs+gamma*max(Qobs(currY,currX,:)) - Qobs(prvY,prvX,action));
            Qend(prvY,prvX,action) = Qend(prvY,prvX,action) + ...
                alpha*(isend+gamma*max(Qend(currY,currX,:)) - Qend(prvY,prvX,action));
            
            sidewalk(currY,currX) = 80;
            
            imagesc(sidewalk);
            
            
            
        end
        coop_scores{iter} = score;

        
        coop_paths{iter} = pathk;
        fprintf('mode %d - episode %d \n', iter)
        
    end
end

